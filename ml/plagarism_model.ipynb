{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f56a6b-4d43-4b6f-ad3f-b8a908cf1a87",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc234026-d12d-4aac-a9cf-f50791ac8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2365978-23b5-47bb-b37d-ea6070ccbdf2",
   "metadata": {},
   "source": [
    "**Resolving NLTK Error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8858929e-2f61-4250-a67d-3bf90a71dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Manas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35426e16-624b-4448-8f9f-0c09001296f1",
   "metadata": {},
   "source": [
    "**Creating text1 & text2 variables for testing purpose.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73f40084-a095-410c-bf8c-7e06d1617f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Fundamental areas of computer science Computer science is the study of computation, information, and automation. Computer science spans theoretical disciplines to applied disciplines.\"\n",
    "text2 = \"Science is a rigorous, systematic endeavor that builds and organizes knowledge in the form of testable explanations and predictions about everything.\"\n",
    "combined_text = [text1, text2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e941e9-3550-4f56-937e-8e99a97c5bcf",
   "metadata": {},
   "source": [
    "**Function Used To Convert Tokenized List to String.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78ed696e-8c7e-449d-8e77-f09a42792a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):\n",
    "   # initialize an empty string\n",
    "   str1 = \" \"\n",
    "   return (str1.join(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64683da8-6601-431c-8a04-ddb27cd86697",
   "metadata": {},
   "source": [
    "**Function Used To Tokenize and String and Remove StopWords from it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "026adc6f-1073-4d0e-b11d-4b6e4d11cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_remover(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # converts the words in word_tokens to lower case and then checks whether\n",
    "    #they are present in stop_words or not\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    #with no lower case conversion\n",
    "    filtered_sentence = []\n",
    " \n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    " \n",
    "    return listToString(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d30bd36-b1f4-483d-8a2d-1f4df17333fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text1 = \"Science is a rigorous, systematic endeavor that builds and organizes knowledge in the form of testable explanations and predictions about everything.\"\n",
    "# text2 = \"Science is a rigorous, systematic endeavor that builds and organizes knowledge in the form of testable explanations and predictions about everything.\"\n",
    "# combined_text = [text1, text2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6616645-40e7-43cf-aec7-b37923beec79",
   "metadata": {},
   "source": [
    "**Function Used To Vectorize the Text for checking the cosine similarity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7729530-bf9d-4de7-8c03-093522ce5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text):\n",
    "    vectorizer = TfidfVectorizer().fit(text)\n",
    "    return vectorizer.transform(text)\n",
    "    # return TfidfVectorizer.fit_transform(text)\n",
    "    # return TfidfVectorizer.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de2600-e5cc-4da8-9801-b3fa1f7e3cc2",
   "metadata": {},
   "source": [
    "**Function Used To Calculate the Cosine Similarity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d968340-c0eb-4302-9bbe-10d79e0577e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(text1, text2):\n",
    "    return cosine_similarity([text1,text2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658dd7fe-614e-4b2d-899a-b5c9201e94a5",
   "metadata": {},
   "source": [
    "**The Supreme Plagarism Checker Function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74ff02c0-d593-4ed6-8f89-7eebd1725e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plagarism_checker(text1, text2):\n",
    "    text1 = stopwords_remover(text1)\n",
    "    text2 = stopwords_remover(text2)\n",
    "    combined_text = [text1, text2]\n",
    "    vectors = vectorize(combined_text)\n",
    "    a = vectors[0]\n",
    "    b = vectors[1]\n",
    "    # cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    print(cosine_similarity(a,b)[0][0]*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
